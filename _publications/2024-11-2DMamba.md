---
title: "2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification"
collection: publications
category: conferences
permalink: /publication/2024-12-2DMamba
excerpt: '2DMamba is an efficient state space model for image representation, which extends 1D Mamba into 2D while maintaining its modeling capabilities, high parallelism, and memory access efficiency.'
date: 2024-12-01
venue: 'Proceedings of 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)'
paperurl: 'https://arxiv.org/abs/2412.00678'
citation: '<strong>Jingwei Zhang</strong>*, Anh Tien Nguyen*, Xi Han*, Vincent Quoc-Huy Trinh, Hong Qin, Dimitris Samaras, and Mahdi S. Hosseini, &quot;2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification&quot;, In <i>Proceedings of the 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2025.'
figure: /files/images/publications/2024-11-2DMamba.jpg
---

**Abstract**. Efficiently modeling large 2D contexts is essential for various fields including Giga-Pixel Whole Slide Imaging (WSI) and remote sensing. Transformer-based models offer high parallelism but face challenges due to their quadratic complexity for handling long sequences. Recently, Mamba introduced a selective State Space Model (SSM) with linear complexity and high parallelism, enabling effective and efficient modeling of wide context in 1D sequences. However, extending Mamba to vision tasks, which inherently involve 2D structures, results in spatial discrepancies due to the limitations of 1D sequence processing. On the other hand, current 2D SSMs inherently model 2D structures but they suffer from prohibitively slow computation due to the lack of efficient parallel algorithms. In this work, we propose 2DMamba, a novel 2D selective SSM framework that incorporates the 2D spatial structure of images into Mamba, with a highly optimized hardware-aware operator, adopting both spatial continuity and computational efficiency. We validate the versatility of our approach on both WSIs and natural images. Extensive experiments on 10 public datasets for WSI classification and survival analysis show that 2DMamba improves up to 2.48% in AUC, 3.11% in F1 score, 2.47% in accuracy and 5.52% in C-index. Additionally, integrating our method with VMamba for natural imaging yields 0.5 to 0.7 improvements in mIoU on the ADE20k semantic segmentation dataset, and 0.2% accuracy improvement on ImageNet-1K classification dataset. Our code is available at [https://github.com/AtlasAnalyticsLab/2DMamba](https://github.com/AtlasAnalyticsLab/2DMamba). 

More information are available at [CVPR](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_2DMamba_Efficient_State_Space_Model_for_Image_Representation_with_Applications_CVPR_2025_paper.html), [arXiv](https://arxiv.org/abs/2412.00678) and [GitHub](https://github.com/AtlasAnalyticsLab/2DMamba). 
