---
title: "LBMamba: Locally Bi-directional Mamba"
collection: publications
category: journals
permalink: /publication/2025-06-LBMamba
excerpt: 'We present LBMamba, a locally bi-directional State Space Model block that integrates a lightweight, register-level backward scan within the forward scan to restore a full receptive field without the computational overhead of a secondary global backward pass.'
date: 2025-06-19
venue: 'Transactions on Machine Learning Research (TMLR)'
paperurl: 'https://arxiv.org/abs/2506.15976'
citation: '<strong>Jingwei Zhang</strong>*, Xi Han*, Hong Qin, Mahdi S. Hosseini, and Dimitris Samaras, &quot;LBMamba: Locally Bi-directional Mamba&quot;, <i>Transactions on Machine Learning Research (TMLR)</i>, 2025.'
figure: /files/images/publications/2025-06-LBMamba.jpg
---

**Abstract**. Mamba, a State Space Model (SSM) that accelerates training by recasting recurrence as a parallel selective scan, has recently emerged as a linearly-scaling, efficient alternative to self-attention. Because of its unidirectional nature, each state in Mamba only has information of its previous states and is blind to states after. Current Mamba-based computer-vision methods typically overcome this limitation by augmenting Mamba's global forward scan with a global backward scan, forming a bi-directional scan that restores a full receptive field. However, this operation doubles the computational load, eroding much of the efficiency advantage that originally Mamba have. To eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM block that embeds a lightweight locally backward scan inside the forward selective scan and executes it entirely in per-thread registers. Building on LBMamba, we present LBVim, a scalable vision backbone that alternates scan directions every two layers to recover a global receptive field without extra backward sweeps. We validate the versatility of our approach on both natural images and whole slide images (WSIs). We show that our LBVim constantly offers a superior performance-throughput trade-off. That is under the same throughput, LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection dataset. We also integrate LBMamba into the SOTA pathology multiple instance learning (MIL) approach, MambaMIL, which uses single directional scan. Experiments on 3 public WSI classification datasets for show that our method achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1, 1.67% better accuracy.

More information are available at [openreview](https://openreview.net/forum?id=e1aXaIXblQ), [arXiv](https://arxiv.org/abs/2506.15976), and [GitHub](https://github.com/cvlab-stonybrook/LBMamba). 
